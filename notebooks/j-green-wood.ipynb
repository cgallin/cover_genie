{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture draft: \n",
    "\n",
    "cover_genie\n",
    ". \n",
    "-- raw_data\n",
    "-- notebooks\n",
    "-- .gitignore\n",
    "-- .env\n",
    "-- .envrc\n",
    "-- requirements.txt\n",
    "\n",
    "--- bert \n",
    "__init__.py\n",
    "pre_proc_linkedin.py\n",
    "bert_model.py \n",
    "\n",
    "--- recommendation\n",
    "__init__.py\n",
    "pull_jobpostings.py\n",
    "pre_proc_jobs.py\n",
    "rec_model.py\n",
    "\n",
    "--- openai\n",
    "__init__.py\n",
    "pdf_preproc.py\n",
    "lang.py\n",
    "prompt.py\n",
    "\n",
    "--- cg_prod\n",
    "- cg_api\n",
    "__init__.py \n",
    "fast_api.py\n",
    "\n",
    "-cg_interface\n",
    "app.py\n",
    "- pages \n",
    "requirements.txt\n",
    "\n",
    "Dockerfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from io import StringIO\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 15:10:52.740 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.803 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/juliagreenwood/.pyenv/versions/cover_genie/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-12-03 15:10:52.804 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.804 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.804 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.804 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.806 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.806 Session state does not function when running a script without `streamlit run`\n",
      "2024-12-03 15:10:52.806 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.806 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.807 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.807 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.807 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.807 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.807 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.808 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.808 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.808 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.808 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.809 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.809 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.809 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.809 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.810 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.810 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.810 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.810 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.810 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.811 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.811 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.811 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.811 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-03 15:10:52.811 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'getvalue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m user_cv \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mfile_uploader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpload your CV in PDF format: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m'\u001b[39m], accept_multiple_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_cv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# To read file as bytes:\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     bytes_data \u001b[38;5;241m=\u001b[39m \u001b[43muser_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetvalue\u001b[49m() \u001b[38;5;66;03m# not sure about this bit, how to make it work with the pdf text extractor\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     st\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename:\u001b[39m\u001b[38;5;124m'\u001b[39m, user_cv\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m     21\u001b[0m industries \u001b[38;5;241m=\u001b[39m get_select_industries()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'getvalue'"
     ]
    }
   ],
   "source": [
    "# User input page\n",
    "\n",
    "st.markdown(''' # Cover Genie üßû‚Äç‚ôÄÔ∏è''')\n",
    "st.markdown(''' # Automate your job search: Generate Cover letters with ease!''')\n",
    "\n",
    "st.markdown(''' Please enter the following information to generate relevant job postings:''')\n",
    "\n",
    "url = 'https://X.X.X/recommend'\n",
    "\n",
    "if st.button('Recommend jobs'):\n",
    "    pred = requests.get(url, params=query_params).json()\n",
    "    # pred[\"fare\"] = pred[\"fare\"]\n",
    "    # st.markdown(f'This ride will cost you: ${pred[\"fare\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input function\n",
    "def input():\n",
    "    job_title_1 = st.text_input('Enter a desired job title: '),\n",
    "    job_title_2 = st.text_input('Enter a desired job title: '),\n",
    "    job_title_3 = st.text_input('Enter a desired job title: '),\n",
    "\n",
    "    location = st.text_input('Enter the desired work location: '),\n",
    "\n",
    "    user_cv = st.file_uploader('Upload your CV in PDF format: ', type=['pdf'], accept_multiple_files=False),\n",
    "\n",
    "    if user_cv is not None:\n",
    "        # To read file as bytes:\n",
    "        bytes_data = user_cv.getvalue() # not sure about this bit, how to make it work with the pdf text extractor\n",
    "        st.write('filename:', user_cv.name)\n",
    "\n",
    "    industries = get_select_industries()\n",
    "\n",
    "    # Getting params to generate job recommendations,\n",
    "    query_params = {\n",
    "        'job_title_1': job_title_1,\n",
    "        'job_title_2': job_title_2,\n",
    "        'job_title_3': job_title_3,\n",
    "        'location': location,\n",
    "        'user_cv': bytes_data,\n",
    "        'industries': industries,\n",
    "    }\n",
    "    return query_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Job industry\n",
    "\n",
    "industries = ['Healthcare and Biotechnology',\n",
    "            'Technology',\n",
    "            'Manufacturing',\n",
    "            'Consumer Goods and Retail',\n",
    "            'Finance, Banking, Insurance and Accounting',\n",
    "            'Staffing and Recruiting',\n",
    "            'Financial Services',\n",
    "            'Hospitality, Travel, and Food Service',\n",
    "            'Education and Research',\n",
    "            'Construction and Real Estate Development',\n",
    "            'Legal and Consulting Services',\n",
    "            'Transportation and Logistics',\n",
    "            'Real Estate, Property Management, and Construction',\n",
    "            'Government and Public Administration',\n",
    "            'Entertainment and Media',\n",
    "            'Advertising Services',\n",
    "            'Wellness and Fitness Services',\n",
    "            'Environmental and Renewable Energy',\n",
    "            'Utilities']\n",
    "\n",
    "\n",
    "def get_select_industries():\n",
    "    st.write('Select the relevant industries for your job search: ')\n",
    "\n",
    "    selected_industries = []\n",
    "\n",
    "    for industry in industries:\n",
    "        st.checkbox(industry)\n",
    "        if industry:\n",
    "            selected_industries.append(industry)\n",
    "\n",
    "    st.write('You selected:', selected_industries)\n",
    "    return selected_industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output from recommendation system: displaying job recommendations in a dataframe.\n",
    "\n",
    "def get_dataframe_data():\n",
    "\n",
    "    return pd.DataFrame(\n",
    "# data frame with job recommendations: Job title, company, job description, apply button\n",
    "        )\n",
    "\n",
    "df = get_dataframe_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cover letter page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/juliagreenwood/.pyenv/versions/3.10.6/envs/cover_genie/lib/python3.10/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/juliagreenwood/.pyenv/versions/3.10.6/envs/cover_genie/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/juliagreenwood/.pyenv/versions/3.10.6/envs/cover_genie/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Users/juliagreenwood/.pyenv/versions/3.10.6/envs/cover_genie/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/juliagreenwood/.pyenv/versions/3.10.6/envs/cover_genie/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/juliagreenwood/.pyenv/versions/3.10.6/envs/cover_genie/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/juliagreenwood/.pyenv/versions/3.10.6/envs/cover_genie/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/juliagreenwood/.pyenv/versions/3.10.6/envs/cover_genie/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/juliagreenwood/.pyenv/versions/3.10.6/envs/cover_genie/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading safetensors-0.4.5-cp310-cp310-macosx_11_0_arm64.whl (381 kB)\n",
      "Downloading tokenizers-0.20.3-cp310-cp310-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.26.3 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.20.3 tqdm-4.67.1 transformers-4.46.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /Users/juliagreenwood/.pyenv/versions/3.10.6/envs/cover_genie/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/juliagreenwood/.pyenv/versions/3.10.6/envs/cover_genie/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/juliagreenwood/.pyenv/versions/3.10.6/envs/cover_genie/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/juliagreenwood/.pyenv/versions/3.10.6/envs/cover_genie/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/juliagreenwood/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliagreenwood/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/juliagreenwood/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "import pandas as pd\n",
    "from nltk import pos_tag, sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from nltk.probability import FreqDist\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/juliagreenwood/code/cgallin/cover_genie/notebooks\n",
      "New Working Directory: /Users/juliagreenwood/code/cgallin/cover_genie\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Change the working directory\n",
    "new_dir = \"/Users/juliagreenwood/code/cgallin/cover_genie\"\n",
    "os.chdir(new_dir)\n",
    "\n",
    "# Verify the new working directory\n",
    "print(\"New Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.pdf_preproc import pdf_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cv = 'CV_Julia Greenwood_09.10.22.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdf_to_text(user_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pdf to string\n",
    "# pre-process the resume data\n",
    "# pr-process the job description data\n",
    "# embedding\n",
    "# Vectorize the resume and job description\n",
    "# cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    sentences = sent_tokenize(text)\n",
    "    features = {'feature': \"\"}\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words = set(stopwords.words(\"french\"))\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        tagged_words = pos_tag(words)\n",
    "        filtered_words = [word for word, tag in tagged_words if tag not in ['DT', 'IN', 'TO', 'PRP', 'WP']]\n",
    "        features['feature'] += \" \".join(filtered_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resume_data(df):\n",
    "    id = df['ID']\n",
    "    category = df['Category']\n",
    "    text = extract_text_from_pdf(f\"/kaggle/input/resume-dataset/data/data/{category}/{id}.pdf\")\n",
    "    features = preprocess_text(text)\n",
    "    df['Feature'] = features['feature']\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text, model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().to(\"cpu\").numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine all text data for fitting the vectorizer\n",
    "corpus = list(resume_data['Feature']) + list(job_descriptions2['jdFeatures'])\n",
    "\n",
    "# Initialize and fit TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2))\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Split the TF-IDF matrix into resumes and job descriptions\n",
    "resume_tfidf = tfidf_matrix[:len(resume_data)]\n",
    "job_tfidf = tfidf_matrix[len(resume_data):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfidf_similarity_matrix = cosine_similarity(job_tfidf, resume_tfidf)\n",
    "\n",
    "# Get top 5 resumes for each job description\n",
    "for job_idx in range(5,15):\n",
    "    top_resumes = tfidf_similarity_matrix[job_idx].argsort()[::-1][:5]\n",
    "    print(f\"Job: {job_descriptions2['title'].iloc[job_idx]}\")\n",
    "    for idx in top_resumes:\n",
    "        print(f\"  Resume ID: {resume_data['ID'].iloc[idx]}, Similarity: {tfidf_similarity_matrix[job_idx][idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change input location : from text input to choice between Montreal and Toronto"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cover_genie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
