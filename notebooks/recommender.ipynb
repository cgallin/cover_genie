{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jacobemerson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jacobemerson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jacobemerson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jacobemerson/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>max_salary</th>\n",
       "      <th>pay_period</th>\n",
       "      <th>location</th>\n",
       "      <th>company_id</th>\n",
       "      <th>views</th>\n",
       "      <th>med_salary</th>\n",
       "      <th>...</th>\n",
       "      <th>skills_desc</th>\n",
       "      <th>listed_time</th>\n",
       "      <th>posting_domain</th>\n",
       "      <th>sponsored</th>\n",
       "      <th>work_type</th>\n",
       "      <th>currency</th>\n",
       "      <th>compensation_type</th>\n",
       "      <th>normalized_salary</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>fips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>921716</td>\n",
       "      <td>Corcoran Sawyer Smith</td>\n",
       "      <td>Marketing Coordinator</td>\n",
       "      <td>Job descriptionA leading real estate firm in N...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>HOURLY</td>\n",
       "      <td>Princeton, NJ</td>\n",
       "      <td>2774458.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Requirements: \\n\\nWe are seeking a College or ...</td>\n",
       "      <td>1.713398e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "      <td>38480.0</td>\n",
       "      <td>8540.0</td>\n",
       "      <td>34021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1829192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mental Health Therapist/Counselor</td>\n",
       "      <td>At Aspen Therapy and Wellness , we are committ...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>HOURLY</td>\n",
       "      <td>Fort Collins, CO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.712858e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "      <td>83200.0</td>\n",
       "      <td>80521.0</td>\n",
       "      <td>8069.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10998357</td>\n",
       "      <td>The National Exemplar</td>\n",
       "      <td>Assitant Restaurant Manager</td>\n",
       "      <td>The National Exemplar is accepting application...</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>YEARLY</td>\n",
       "      <td>Cincinnati, OH</td>\n",
       "      <td>64896719.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>We are currently accepting resumes for FOH - A...</td>\n",
       "      <td>1.713278e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>45202.0</td>\n",
       "      <td>39061.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23221523</td>\n",
       "      <td>Abrams Fensterman, LLP</td>\n",
       "      <td>Senior Elder Law / Trusts and Estates Associat...</td>\n",
       "      <td>Senior Associate Attorney - Elder Law / Trusts...</td>\n",
       "      <td>175000.0</td>\n",
       "      <td>YEARLY</td>\n",
       "      <td>New Hyde Park, NY</td>\n",
       "      <td>766262.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>This position requires a baseline understandin...</td>\n",
       "      <td>1.712896e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>11040.0</td>\n",
       "      <td>36059.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35982263</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Service Technician</td>\n",
       "      <td>Looking for HVAC service tech with experience ...</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>YEARLY</td>\n",
       "      <td>Burlington, IA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.713452e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>52601.0</td>\n",
       "      <td>19057.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     job_id            company_name  \\\n",
       "0    921716   Corcoran Sawyer Smith   \n",
       "1   1829192                     NaN   \n",
       "2  10998357  The National Exemplar    \n",
       "3  23221523  Abrams Fensterman, LLP   \n",
       "4  35982263                     NaN   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Marketing Coordinator   \n",
       "1                  Mental Health Therapist/Counselor   \n",
       "2                        Assitant Restaurant Manager   \n",
       "3  Senior Elder Law / Trusts and Estates Associat...   \n",
       "4                                 Service Technician   \n",
       "\n",
       "                                         description  max_salary pay_period  \\\n",
       "0  Job descriptionA leading real estate firm in N...        20.0     HOURLY   \n",
       "1  At Aspen Therapy and Wellness , we are committ...        50.0     HOURLY   \n",
       "2  The National Exemplar is accepting application...     65000.0     YEARLY   \n",
       "3  Senior Associate Attorney - Elder Law / Trusts...    175000.0     YEARLY   \n",
       "4  Looking for HVAC service tech with experience ...     80000.0     YEARLY   \n",
       "\n",
       "            location  company_id  views  med_salary  ...  \\\n",
       "0      Princeton, NJ   2774458.0   20.0         NaN  ...   \n",
       "1   Fort Collins, CO         NaN    1.0         NaN  ...   \n",
       "2     Cincinnati, OH  64896719.0    8.0         NaN  ...   \n",
       "3  New Hyde Park, NY    766262.0   16.0         NaN  ...   \n",
       "4     Burlington, IA         NaN    3.0         NaN  ...   \n",
       "\n",
       "                                         skills_desc   listed_time  \\\n",
       "0  Requirements: \\n\\nWe are seeking a College or ...  1.713398e+12   \n",
       "1                                                NaN  1.712858e+12   \n",
       "2  We are currently accepting resumes for FOH - A...  1.713278e+12   \n",
       "3  This position requires a baseline understandin...  1.712896e+12   \n",
       "4                                                NaN  1.713452e+12   \n",
       "\n",
       "   posting_domain  sponsored  work_type currency compensation_type  \\\n",
       "0             NaN          0  FULL_TIME      USD       BASE_SALARY   \n",
       "1             NaN          0  FULL_TIME      USD       BASE_SALARY   \n",
       "2             NaN          0  FULL_TIME      USD       BASE_SALARY   \n",
       "3             NaN          0  FULL_TIME      USD       BASE_SALARY   \n",
       "4             NaN          0  FULL_TIME      USD       BASE_SALARY   \n",
       "\n",
       "  normalized_salary  zip_code     fips  \n",
       "0           38480.0    8540.0  34021.0  \n",
       "1           83200.0   80521.0   8069.0  \n",
       "2           55000.0   45202.0  39061.0  \n",
       "3          157500.0   11040.0  36059.0  \n",
       "4           70000.0   52601.0  19057.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_df = pd.read_csv(\"/Users/jacobemerson/code/cgallin/cover_genie/raw_data/job_data/postings.csv\")\n",
    "jobs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessor(df):\n",
    "#     def text_cleaner(df):\n",
    "#         # Ensure 'description' column exists\n",
    "#         if 'description' not in df.columns:\n",
    "#             raise ValueError(\"DataFrame must contain a 'description' column.\")\n",
    "        \n",
    "#         # Drop rows with missing 'description' values\n",
    "#         df_cleaned = df.dropna(subset=['description'])\n",
    "        \n",
    "#         # Convert text to lowercase and remove punctuation\n",
    "#         translator = str.maketrans('', '', string.punctuation)\n",
    "#         df_cleaned['description'] = df_cleaned['description'].str.lower().str.translate(translator)\n",
    "        \n",
    "#         # Remove numbers\n",
    "#         df_cleaned['description'] = df_cleaned['description'].str.replace(r'\\d+', '', regex=True)\n",
    "        \n",
    "#         return df_cleaned['description']\n",
    "\n",
    "#     text = text_cleaner(df)\n",
    "\n",
    "#     # Tokenize the text\n",
    "#     def tokenizer(texts):\n",
    "#         return texts.apply(word_tokenize)\n",
    "\n",
    "#     tokens = tokenizer(text)\n",
    "\n",
    "#     # Remove stopwords\n",
    "#     def remove_stopwords(tokens):\n",
    "#         stop_words = set(stopwords.words('english')).union(stopwords.words('french'))\n",
    "#         return tokens.apply(lambda words: [w for w in words if w not in stop_words])\n",
    "\n",
    "#     no_stopword_tokens = remove_stopwords(tokens)\n",
    "\n",
    "#     # Lemmatize text\n",
    "#     def lemmatize(tokens):\n",
    "#         lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "#         def lemmatize_words(words):\n",
    "#             # Lemmatize verbs and nouns\n",
    "#             lemmatized_verbs = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "#             lemmatized_nouns = [lemmatizer.lemmatize(word, pos='n') for word in lemmatized_verbs]\n",
    "#             return lemmatized_nouns\n",
    "        \n",
    "#         return tokens.apply(lemmatize_words)\n",
    "\n",
    "#     lemmatized_tokens = lemmatize(no_stopword_tokens)\n",
    "\n",
    "#     # Return lemmatized tokens as a new DataFrame column\n",
    "#     df['processed_description'] = lemmatized_tokens\n",
    "#     return pd.DataFrame(df['processed_description'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessor(df):\n",
    "#     # Clean and preprocess text\n",
    "#     def text_cleaner(df):\n",
    "#         if 'description' not in df.columns:\n",
    "#             raise ValueError(\"DataFrame must contain a 'description' column.\")\n",
    "        \n",
    "#         # Drop rows with missing descriptions\n",
    "#         df_cleaned = df.dropna(subset=['description']).copy()\n",
    "        \n",
    "#         # Apply regex to clean text (keep only letters, replace others with space)\n",
    "#         df_cleaned['description'] = df_cleaned['description'].apply(lambda x: re.sub('[^a-zA-Z]', ' ', x))\n",
    "        \n",
    "#         # Convert to lowercase\n",
    "#         df_cleaned['description'] = df_cleaned['description'].str.lower()\n",
    "        \n",
    "#         return df_cleaned['description']\n",
    "\n",
    "#     # Process sentences within the cleaned text\n",
    "#     def process_sentences(texts):\n",
    "#         # Set up stopwords and lemmatizer\n",
    "#         stop_words = set(stopwords.words('english')).union(stopwords.words('french'))\n",
    "#         lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "#         # Extract features from sentences\n",
    "#         def extract_features(text):\n",
    "#             features = {'feature': \"\"}\n",
    "#             sentences = sent_tokenize(text)\n",
    "#             for sent in sentences:\n",
    "#                 # Tokenize, remove stopwords, and filter by POS tags\n",
    "#                 words = word_tokenize(sent)\n",
    "#                 words = [word for word in words if word not in stop_words]\n",
    "#                 tagged_words = pos_tag(words)\n",
    "#                 filtered_words = [word for word, tag in tagged_words if tag not in ['DT', 'IN', 'TO', 'PRP', 'WP']]\n",
    "                \n",
    "#                 # Lemmatize remaining words\n",
    "#                 lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in filtered_words]\n",
    "#                 lemmatized_words = [lemmatizer.lemmatize(word, pos='n') for word in lemmatized_words]\n",
    "                \n",
    "#                 # Append to features\n",
    "#                 features['feature'] += \" \".join(lemmatized_words) + \" \"\n",
    "#             return features['feature']\n",
    "        \n",
    "#         # Apply feature extraction to all texts\n",
    "#         return texts.apply(extract_features)\n",
    "    \n",
    "#     # Apply text cleaning\n",
    "#     cleaned_text = text_cleaner(df)\n",
    "    \n",
    "#     # Process sentences and extract features\n",
    "#     processed_text = process_sentences(cleaned_text)\n",
    "    \n",
    "#     # Add processed text as a new column\n",
    "#     df['processed_description'] = processed_text\n",
    "#     return df[['processed_description']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    # Clean the text\n",
    "    def text_cleaner(text):\n",
    "        # Apply regex to clean text (keep only letters, replace others with space)\n",
    "        cleaned_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        cleaned_text = cleaned_text.lower()\n",
    "        \n",
    "        return cleaned_text\n",
    "\n",
    "    # Process sentences within the cleaned text\n",
    "    def process_sentences(text):\n",
    "        # Set up stopwords and lemmatizer\n",
    "        stop_words = set(stopwords.words('english')).union(stopwords.words('french'))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Extract features from sentences\n",
    "        def extract_features(text):\n",
    "            features = \"\"\n",
    "            sentences = sent_tokenize(text)\n",
    "            for sent in sentences:\n",
    "                # Tokenize, remove stopwords, and filter by POS tags\n",
    "                words = word_tokenize(sent)\n",
    "                words = [word for word in words if word not in stop_words]\n",
    "                tagged_words = pos_tag(words)\n",
    "                filtered_words = [word for word, tag in tagged_words if tag not in ['DT', 'IN', 'TO', 'PRP', 'WP']]\n",
    "                \n",
    "                # Lemmatize remaining words\n",
    "                lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in filtered_words]\n",
    "                lemmatized_words = [lemmatizer.lemmatize(word, pos='n') for word in lemmatized_words]\n",
    "                \n",
    "                # Append to features\n",
    "                features += \" \".join(lemmatized_words) + \" \"\n",
    "            return features.strip()\n",
    "        \n",
    "        # Apply feature extraction to the text\n",
    "        return extract_features(text)\n",
    "    \n",
    "    # Apply text cleaning\n",
    "    cleaned_text = text_cleaner(text)\n",
    "    \n",
    "    # Process sentences and extract features\n",
    "    processed_text = process_sentences(cleaned_text)\n",
    "    \n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessor function to the first 100 rows of the 'description' column\n",
    "jobs_df.loc[:99, 'processed_description'] = jobs_df.loc[:99, 'description'].apply(preprocessor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes_df = pd.read_csv('/Users/jacobemerson/code/cgallin/cover_genie/raw_data/archive-3/Resume/Resume.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes_df = resumes_df.rename(columns={'Resume_str':'description'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes_df.loc[:99, 'processed_description'] = resumes_df.loc[:99, 'description'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine all text data for fitting the vectorizer\n",
    "# resumes = resumes_df.loc[:99, 'processed_description'].str.split().sum()\n",
    "# job_desc = jobs_df.loc[:99, 'processed_description'].str.split().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Vectorizing the text data\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# job_desc_tfidf = vectorizer.fit_transform(resumes_df.loc[:99, 'processed_description'])\n",
    "# resume_tfidf = vectorizer.transform(jobs_df.loc[:99, 'processed_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# tfidf_similarity_matrix = cosine_similarity(job_desc_tfidf, resume_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the number of top jobs to review\n",
    "# k = 5\n",
    "\n",
    "# for resume_idx in range(tfidf_similarity_matrix.shape[1]):  # Loop over resumes\n",
    "#     top_jobs = tfidf_similarity_matrix[:, resume_idx].argsort()[::-1][:k]\n",
    "#     print(f\"Resume {resume_idx}: Top {k} jobs\")\n",
    "#     for job_idx in top_jobs:\n",
    "#         similarity_score = tfidf_similarity_matrix[job_idx, resume_idx]\n",
    "#         print(f\"  Job {job_idx}, Similarity: {similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# def recommendation(resume_text, job_text, k=5):\n",
    "#     \"\"\"\n",
    "#     Recommends top k jobs for each resume based on text similarity.\n",
    "\n",
    "#     Parameters:\n",
    "#     - resume_text: List or Series of resume descriptions (processed).\n",
    "#     - job_text: List or Series of job descriptions (processed).\n",
    "#     - k: Number of top job recommendations for each resume.\n",
    "\n",
    "#     Returns:\n",
    "#     - None (prints recommendations for each resume).\n",
    "#     \"\"\"\n",
    "#     # Ensure inputs are lists of strings\n",
    "#     if isinstance(resume_text, list):\n",
    "#         resume_text = pd.Series(resume_text)\n",
    "#     if isinstance(job_text, list):\n",
    "#         job_text = pd.Series(job_text)\n",
    "\n",
    "#     # Vectorizing the text data\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     job_desc_tfidf = vectorizer.fit_transform(job_text)\n",
    "#     resume_tfidf = vectorizer.transform(resume_text)\n",
    "\n",
    "#     # Compute the cosine similarity between job descriptions and resumes\n",
    "#     tfidf_similarity_matrix = cosine_similarity(job_desc_tfidf, resume_tfidf)\n",
    "\n",
    "#     # Iterate through each resume and find the top k most similar jobs\n",
    "#     for resume_idx in range(tfidf_similarity_matrix.shape[1]):  # Loop over resumes\n",
    "#         top_jobs = tfidf_similarity_matrix[:, resume_idx].argsort()[::-1][:k]\n",
    "#         print(f\"Resume {resume_idx}: Top {k} jobs\")\n",
    "#         for job_idx in top_jobs:\n",
    "#             similarity_score = tfidf_similarity_matrix[job_idx, resume_idx]\n",
    "#             print(f\"  Job {job_idx}, Similarity: {similarity_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# def recommendation(resume_text, job_text, k=5):\n",
    "#     \"\"\"\n",
    "#     Recommends top k jobs for a given resume based on text similarity.\n",
    "\n",
    "#     Parameters:\n",
    "#     - resume_text: A single resume description (processed).\n",
    "#     - job_text: List or Series of job descriptions (processed).\n",
    "#     - k: Number of top job recommendations.\n",
    "\n",
    "#     Returns:\n",
    "#     - List of tuples containing job indices and similarity scores for the top k jobs.\n",
    "#     \"\"\"\n",
    "#     # Ensure inputs are in the correct format\n",
    "#     if isinstance(job_text, list):\n",
    "#         job_text = pd.Series(job_text)\n",
    "\n",
    "#     # Vectorizing the text data\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     job_desc_tfidf = vectorizer.fit_transform(job_text)\n",
    "#     resume_tfidf = vectorizer.transform([resume_text])  # Transform single resume\n",
    "\n",
    "#     # Compute the cosine similarity between the resume and all job descriptions\n",
    "#     tfidf_similarity_scores = cosine_similarity(job_desc_tfidf, resume_tfidf).flatten()\n",
    "\n",
    "#     # Find the top k most similar jobs\n",
    "#     top_jobs = tfidf_similarity_scores.argsort()[::-1][:k]\n",
    "#     top_job_scores = [(job_idx, tfidf_similarity_scores[job_idx]) for job_idx in top_jobs]\n",
    "\n",
    "#     return top_job_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# def recommendation(resume_text, job_text, df, k=5):\n",
    "#     \"\"\"\n",
    "#     Recommends top k jobs for a given resume based on text similarity.\n",
    "\n",
    "#     Parameters:\n",
    "#     - resume_text: A single resume description or list of resume descriptions (processed).\n",
    "#     - job_text: A list or Series of job descriptions (processed).\n",
    "#     - k: Number of top job recommendations.\n",
    "\n",
    "#     Returns:\n",
    "#     - List of tuples containing job indices and similarity scores for the top k jobs.\n",
    "#     \"\"\"\n",
    "#     # Ensure that resume_text is a list of strings\n",
    "#     if isinstance(resume_text, str):  # If it's a single resume string\n",
    "#         resume_text = [resume_text]  # Convert to a list\n",
    "\n",
    "#     if isinstance(job_text, pd.Series):  # If job_text is a pandas Series\n",
    "#         job_text = job_text.tolist()  # Convert to a list of strings\n",
    "\n",
    "#     # Ensure job_text is a list of strings\n",
    "#     if not isinstance(job_text, list):\n",
    "#         raise ValueError(\"job_text should be a list of strings.\")\n",
    "\n",
    "#     # Vectorizing the text data\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     job_desc_tfidf = vectorizer.fit_transform(job_text)\n",
    "\n",
    "#     # Prepare a list to store top recommendations for each resume\n",
    "#     all_top_job_scores = []\n",
    "\n",
    "#     # Process each resume in resume_text\n",
    "#     for resume in resume_text:\n",
    "#         resume_tfidf = vectorizer.transform([resume])  # Transform the single resume\n",
    "\n",
    "#         # Compute the cosine similarity between the resume and all job descriptions\n",
    "#         tfidf_similarity_scores = cosine_similarity(job_desc_tfidf, resume_tfidf).flatten()\n",
    "\n",
    "#         # Find the top k most similar jobs\n",
    "#         top_jobs = tfidf_similarity_scores.argsort()[::-1][:k]\n",
    "#         top_job_scores = [(job_idx, tfidf_similarity_scores[job_idx]) for job_idx in top_jobs]\n",
    "\n",
    "#         # Append the top job scores for the current resume\n",
    "#         all_top_job_scores.append(top_job_scores)\n",
    "    \n",
    "#     indices = [job[0] for job in all_top_job_scores]\n",
    "        \n",
    "#     def index_rows_by_values(df, indices):\n",
    "#         \"\"\"\n",
    "#         Indexes the rows of a DataFrame by an array of row indices.\n",
    "\n",
    "#         Parameters:\n",
    "#         - df (pd.DataFrame): The DataFrame to index.\n",
    "#         - indices (list or array-like): A list or array of row indices to select.\n",
    "\n",
    "#         Returns:\n",
    "#         - pd.DataFrame: A new DataFrame with only the selected rows.\n",
    "#         \"\"\"\n",
    "#         if not isinstance(df, pd.DataFrame):\n",
    "#             raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n",
    "        \n",
    "#         if not all(isinstance(i, int) for i in indices):\n",
    "#             raise ValueError(\"All elements in 'indices' must be integers.\")\n",
    "        \n",
    "#         top_jobs_df = df.iloc[indices]\n",
    "        \n",
    "#         return top_jobs_df[['title', 'company_name', 'description']]\n",
    "    \n",
    "#     top_jobs_df = index_rows_by_values(df, indices)\n",
    "\n",
    "#     return top_jobs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommendation(resume_text, job_text, df, k=5):\n",
    "    \"\"\"\n",
    "    Recommends top k jobs for a given resume based on text similarity and retrieves job details.\n",
    "\n",
    "    Parameters:\n",
    "    - resume_text: A single resume description or list of resume descriptions (processed).\n",
    "    - job_text: A list or Series of job descriptions (processed).\n",
    "    - df: A DataFrame containing job details with columns 'title', 'company_name', 'description'.\n",
    "    - k: Number of top job recommendations.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing top job recommendations with 'title', 'company_name', 'description'.\n",
    "    \"\"\"\n",
    "    # Ensure that resume_text is a list of strings\n",
    "    if isinstance(resume_text, str):  # If it's a single resume string\n",
    "        resume_text = [resume_text]  # Convert to a list\n",
    "\n",
    "    if isinstance(job_text, pd.Series):  # If job_text is a pandas Series\n",
    "        job_text = job_text.tolist()  # Convert to a list of strings\n",
    "\n",
    "    # Ensure job_text is a list of strings\n",
    "    if not isinstance(job_text, list):\n",
    "        raise ValueError(\"job_text should be a list of strings.\")\n",
    "\n",
    "    # Vectorizing the text data\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    job_desc_tfidf = vectorizer.fit_transform(job_text)\n",
    "\n",
    "    # Prepare to store all DataFrames for top recommendations\n",
    "    all_top_jobs_df = []\n",
    "\n",
    "    # Process each resume in resume_text\n",
    "    for resume in resume_text:\n",
    "        resume_tfidf = vectorizer.transform([resume])  # Transform the single resume\n",
    "\n",
    "        # Compute the cosine similarity between the resume and all job descriptions\n",
    "        tfidf_similarity_scores = cosine_similarity(job_desc_tfidf, resume_tfidf).flatten()\n",
    "\n",
    "        # Find the top k most similar jobs\n",
    "        top_jobs = tfidf_similarity_scores.argsort()[::-1][:k]\n",
    "\n",
    "        # Index rows from the DataFrame\n",
    "        top_jobs_df = df.iloc[top_jobs][['title', 'company_name', 'description']]\n",
    "\n",
    "        # Append to the list of results\n",
    "        all_top_jobs_df.append(top_jobs_df)\n",
    "\n",
    "    # Return the results as a list of DataFrames (one per resume) or a concatenated DataFrame\n",
    "    return all_top_jobs_df if len(all_top_jobs_df) > 1 else all_top_jobs_df[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first 100 processed descriptions\n",
    "resume_text = resumes_df.loc[:99, 'processed_description']\n",
    "job_text = jobs_df.loc[:99, 'processed_description']\n",
    "\n",
    "\n",
    "# # Iterate through each resume and get recommendations\n",
    "# for idx, resume_text in enumerate(resume_text):\n",
    "#     print(f\"Recommendations for Resume {idx}:\")\n",
    "#     top_jobs = recommendation(resume_text, job_text, k=5)\n",
    "#     for job_idx, similarity in top_jobs:\n",
    "#         print(f\"  Job {job_idx}, Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first 100 processed descriptions\n",
    "resume_text = resumes_df.loc[0, 'processed_description']\n",
    "job_text = jobs_df.loc[:99, 'processed_description']\n",
    "\n",
    "rec = recommendation(resume_text, job_text, jobs_df, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Marketing Coordinator</td>\n",
       "      <td>Lynx Systems</td>\n",
       "      <td>Micro Technology Services Inc. is a leading pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Marketing &amp; Office Coordinator</td>\n",
       "      <td>Revesco Properties</td>\n",
       "      <td>About Revesco Properties:Revesco Properties is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Service Coordinator</td>\n",
       "      <td>Grunwald Mechanical Contractors &amp; Engineers</td>\n",
       "      <td>QualificationsExperience:Data Entry, 4 years (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Senior Product Marketing Manager</td>\n",
       "      <td>Staffing Theory</td>\n",
       "      <td>A leading pharmaceutical company committed to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Administrative Assistant</td>\n",
       "      <td>REquipment Durable Medical Equipment and Assis...</td>\n",
       "      <td>The Administrative Assistant will organize and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  \\\n",
       "99             Marketing Coordinator   \n",
       "35    Marketing & Office Coordinator   \n",
       "95               Service Coordinator   \n",
       "14  Senior Product Marketing Manager   \n",
       "30          Administrative Assistant   \n",
       "\n",
       "                                         company_name  \\\n",
       "99                                       Lynx Systems   \n",
       "35                                 Revesco Properties   \n",
       "95        Grunwald Mechanical Contractors & Engineers   \n",
       "14                                    Staffing Theory   \n",
       "30  REquipment Durable Medical Equipment and Assis...   \n",
       "\n",
       "                                          description  \n",
       "99  Micro Technology Services Inc. is a leading pr...  \n",
       "35  About Revesco Properties:Revesco Properties is...  \n",
       "95  QualificationsExperience:Data Entry, 4 years (...  \n",
       "14  A leading pharmaceutical company committed to ...  \n",
       "30  The Administrative Assistant will organize and...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [job[0] for job in rec]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_rows_by_values(df, indices):\n",
    "    \"\"\"\n",
    "    Indexes the rows of a DataFrame by an array of row indices.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to index.\n",
    "    - indices (list or array-like): A list or array of row indices to select.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A new DataFrame with only the selected rows.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n",
    "    \n",
    "    if not all(isinstance(i, int) for i in indices):\n",
    "        raise ValueError(\"All elements in 'indices' must be integers.\")\n",
    "    \n",
    "    top_jobs_df = df.iloc[indices]\n",
    "    \n",
    "    return top_jobs_df[['title', 'company_name', 'description']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs_df.loc[16, ['description', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(jobs_df['description'][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(resumes_df['description'][99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
