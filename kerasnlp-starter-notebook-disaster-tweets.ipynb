{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\n",
    "This starter notebook is provided by the Keras team.</center>\n",
    "\n",
    "## Keras NLP starter guide here: https://keras.io/guides/keras_nlp/getting_started/\n",
    "\n",
    "In this competition, the challenge is to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t.\n",
    "A dataset of 10,000 tweets that were hand classified is available. \n",
    "\n",
    "__This starter notebook uses the [DistilBERT](https://arxiv.org/abs/1910.01108) pretrained model from KerasNLP.__\n",
    "\n",
    "\n",
    "**BERT** stands for **Bidirectional Encoder Representations from Transformers**. BERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models.\n",
    "\n",
    "The BERT family of models uses the **Transformer encoder architecture** to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers.\n",
    "\n",
    "BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n",
    "\n",
    "**DistilBERT model** is a distilled form of the **BERT** model. The size of a BERT model was reduced by 40% via knowledge distillation during the pre-training phase while retaining 97% of its language understanding abilities and being 60% faster.\n",
    "\n",
    "\n",
    "\n",
    "![BERT Architecture](https://www.cse.chalmers.se/~richajo/nlp2019/l5/bert_class.png)\n",
    "\n",
    "\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "- Load the Disaster Tweets\n",
    "- Explore the dataset\n",
    "- Preprocess the data\n",
    "- Load a DistilBERT model from Keras NLP\n",
    "- Train your own model, fine-tuning BERT\n",
    "- Generate the submission file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-31T20:00:45.041191Z",
     "iopub.status.busy": "2023-07-31T20:00:45.040776Z",
     "iopub.status.idle": "2023-07-31T20:01:10.624768Z",
     "shell.execute_reply": "2023-07-31T20:01:10.623539Z",
     "shell.execute_reply.started": "2023-07-31T20:00:45.041149Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-core in /Users/camerongallinger/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (0.1.7)\n",
      "Requirement already satisfied: absl-py in /Users/camerongallinger/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from keras-core) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Users/camerongallinger/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from keras-core) (2.0.2)\n",
      "Requirement already satisfied: rich in /Users/camerongallinger/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from keras-core) (12.6.0)\n",
      "Requirement already satisfied: namex in /Users/camerongallinger/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from keras-core) (0.0.8)\n",
      "Requirement already satisfied: h5py in /Users/camerongallinger/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from keras-core) (3.12.1)\n",
      "Requirement already satisfied: dm-tree in /Users/camerongallinger/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from keras-core) (0.1.8)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /Users/camerongallinger/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from rich->keras-core) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /Users/camerongallinger/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from rich->keras-core) (2.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-core --upgrade\n",
    "!pip install -q keras-hub --upgrade\n",
    "\n",
    "# This sample uses Keras Core, the multi-backend version of Keras.\n",
    "# The selected backend is TensorFlow (other supported backends are 'jax' and 'torch')\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T20:01:10.62922Z",
     "iopub.status.busy": "2023-07-31T20:01:10.628924Z",
     "iopub.status.idle": "2023-07-31T20:01:20.556625Z",
     "shell.execute_reply": "2023-07-31T20:01:20.555665Z",
     "shell.execute_reply.started": "2023-07-31T20:01:10.629194Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ops' from 'keras' (/Users/camerongallinger/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \u001b[38;5;66;03m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_hub\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_core\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrixDisplay, confusion_matrix\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras_hub/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Import everything from /api/ into keras.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# Import * ignores names start with \"_\".\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Add everything in /api/ to the module search path.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras_hub/api/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bounding_box\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras_hub/api/bounding_box/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbounding_box\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_format\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbounding_box\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CENTER_XYWH\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbounding_box\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m REL_XYWH\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras_hub/src/bounding_box/converters.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Converter functions for working with bounding box formats.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_hub_export\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ops' from 'keras' (/Users/camerongallinger/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import keras_hub\n",
    "import keras_core as keras\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"KerasNLP version:\", keras_hub.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Disaster Tweets\n",
    "Let's have a look at the train and test dataset.\n",
    "\n",
    "They contain:\n",
    "- id\n",
    "- keyword: A keyword from that tweet (although this may be blank!)\n",
    "- location: The location the tweet was sent from (may also be blank)\n",
    "- text: The text of a tweet\n",
    "- target: 1 if the tweet is a real disaster or 0 if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T20:01:20.558472Z",
     "iopub.status.busy": "2023-07-31T20:01:20.557857Z",
     "iopub.status.idle": "2023-07-31T20:01:20.62669Z",
     "shell.execute_reply": "2023-07-31T20:01:20.625756Z",
     "shell.execute_reply.started": "2023-07-31T20:01:20.558435Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "df_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "\n",
    "print('Training Set Shape = {}'.format(df_train.shape))\n",
    "print('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\n",
    "print('Test Set Shape = {}'.format(df_test.shape))\n",
    "print('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T20:01:20.629925Z",
     "iopub.status.busy": "2023-07-31T20:01:20.629651Z",
     "iopub.status.idle": "2023-07-31T20:01:20.646214Z",
     "shell.execute_reply": "2023-07-31T20:01:20.645286Z",
     "shell.execute_reply.started": "2023-07-31T20:01:20.6299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T20:01:20.649765Z",
     "iopub.status.busy": "2023-07-31T20:01:20.649504Z",
     "iopub.status.idle": "2023-07-31T20:01:20.659375Z",
     "shell.execute_reply": "2023-07-31T20:01:20.658296Z",
     "shell.execute_reply.started": "2023-07-31T20:01:20.649741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T20:01:20.661278Z",
     "iopub.status.busy": "2023-07-31T20:01:20.660532Z",
     "iopub.status.idle": "2023-07-31T20:01:20.686752Z",
     "shell.execute_reply": "2023-07-31T20:01:20.685851Z",
     "shell.execute_reply.started": "2023-07-31T20:01:20.661241Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train[\"length\"] = df_train[\"text\"].apply(lambda x : len(x))\n",
    "df_test[\"length\"] = df_test[\"text\"].apply(lambda x : len(x))\n",
    "\n",
    "print(\"Train Length Stat\")\n",
    "print(df_train[\"length\"].describe())\n",
    "print()\n",
    "\n",
    "print(\"Test Length Stat\")\n",
    "print(df_test[\"length\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know more information about the data, you can grab useful information [here](https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n",
    "\n",
    "Note that all the tweets are in english."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T20:01:20.688711Z",
     "iopub.status.busy": "2023-07-31T20:01:20.688377Z",
     "iopub.status.idle": "2023-07-31T20:01:20.694731Z",
     "shell.execute_reply": "2023-07-31T20:01:20.693638Z",
     "shell.execute_reply.started": "2023-07-31T20:01:20.688679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_TRAINING_EXAMPLES = df_train.shape[0]\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.2\n",
    "STEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\n",
    "\n",
    "EPOCHS = 2\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T20:01:20.697023Z",
     "iopub.status.busy": "2023-07-31T20:01:20.696116Z",
     "iopub.status.idle": "2023-07-31T20:01:20.717538Z",
     "shell.execute_reply": "2023-07-31T20:01:20.716665Z",
     "shell.execute_reply.started": "2023-07-31T20:01:20.696979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_train[\"text\"]\n",
    "y = df_train[\"target\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n",
    "\n",
    "X_test = df_test[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a DistilBERT model from Keras NLP\n",
    "\n",
    "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT.\n",
    "\n",
    "The BertClassifier model can be configured with a preprocessor layer, in which case it will automatically apply preprocessing to raw inputs during fit(), predict(), and evaluate(). This is done by default when creating the model with from_preset().\n",
    "\n",
    "We will choose DistilBERT model.that learns a distilled (approximate) version of BERT, retaining 97% performance but using only half the number of parameters ([paper](https://arxiv.org/abs/1910.01108)). \n",
    "\n",
    "It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.\n",
    "\n",
    "Specifically, it doesn't have token-type embeddings, pooler and retains only half of the layers from Google's BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T20:01:20.719649Z",
     "iopub.status.busy": "2023-07-31T20:01:20.719008Z",
     "iopub.status.idle": "2023-07-31T20:01:37.14406Z",
     "shell.execute_reply": "2023-07-31T20:01:37.142842Z",
     "shell.execute_reply.started": "2023-07-31T20:01:20.7196Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras_nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m preset\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistil_bert_base_en_uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Use a shorter sequence length.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m \u001b[43mkeras_nlp\u001b[49m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mDistilBertPreprocessor\u001b[38;5;241m.\u001b[39mfrom_preset(preset,\n\u001b[1;32m      6\u001b[0m                                                                    sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m      7\u001b[0m                                                                    name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor_4_tweets\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m                                                                   )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Pretrained classifier.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m classifier \u001b[38;5;241m=\u001b[39m keras_nlp\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mDistilBertClassifier\u001b[38;5;241m.\u001b[39mfrom_preset(preset,\n\u001b[1;32m     12\u001b[0m                                                                preprocessor \u001b[38;5;241m=\u001b[39m preprocessor, \n\u001b[1;32m     13\u001b[0m                                                                num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras_nlp' is not defined"
     ]
    }
   ],
   "source": [
    "# Load a DistilBERT model.\n",
    "preset= \"distil_bert_base_en_uncased\"\n",
    "\n",
    "# Use a shorter sequence length.\n",
    "preprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset,\n",
    "                                                                   sequence_length=512,\n",
    "                                                                   name=\"preprocessor_4_tweets\"\n",
    "                                                                  )\n",
    "\n",
    "# Pretrained classifier.\n",
    "classifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,\n",
    "                                                               preprocessor = preprocessor,\n",
    "                                                               num_classes=2)\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your own model, fine-tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T20:01:37.146295Z",
     "iopub.status.busy": "2023-07-31T20:01:37.145916Z",
     "iopub.status.idle": "2023-07-31T20:04:19.910429Z",
     "shell.execute_reply": "2023-07-31T20:04:19.909408Z",
     "shell.execute_reply.started": "2023-07-31T20:01:37.146259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "classifier.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), #'binary_crossentropy',\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    metrics= [\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Fit\n",
    "history = classifier.fit(x=X_train,\n",
    "                         y=y_train,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         epochs=EPOCHS,\n",
    "                         validation_data=(X_val, y_val)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T20:04:19.91385Z",
     "iopub.status.busy": "2023-07-31T20:04:19.913491Z",
     "iopub.status.idle": "2023-07-31T20:04:19.923256Z",
     "shell.execute_reply": "2023-07-31T20:04:19.922094Z",
     "shell.execute_reply.started": "2023-07-31T20:04:19.913815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def displayConfusionMatrix(y_true, y_pred, dataset):\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true,\n",
    "        np.argmax(y_pred, axis=1),\n",
    "        display_labels=[\"Not Disaster\",\"Disaster\"],\n",
    "        cmap=plt.cm.Blues\n",
    "    )\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, np.argmax(y_pred, axis=1)).ravel()\n",
    "    f1_score = tp / (tp+((fn+fp)/2))\n",
    "\n",
    "    disp.ax_.set_title(\"Confusion Matrix on \" + dataset + \" Dataset -- F1 Score: \" + str(f1_score.round(2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T20:04:19.925162Z",
     "iopub.status.busy": "2023-07-31T20:04:19.924738Z",
     "iopub.status.idle": "2023-07-31T20:05:04.197288Z",
     "shell.execute_reply": "2023-07-31T20:05:04.19632Z",
     "shell.execute_reply.started": "2023-07-31T20:04:19.925118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred_train = classifier.predict(X_train)\n",
    "\n",
    "displayConfusionMatrix(y_train, y_pred_train, \"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T20:05:04.199632Z",
     "iopub.status.busy": "2023-07-31T20:05:04.198688Z",
     "iopub.status.idle": "2023-07-31T20:05:12.995278Z",
     "shell.execute_reply": "2023-07-31T20:05:12.994338Z",
     "shell.execute_reply.started": "2023-07-31T20:05:04.199591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred_val = classifier.predict(X_val)\n",
    "\n",
    "displayConfusionMatrix(y_val, y_pred_val, \"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the submission file \n",
    "\n",
    "For each tweets in the test set, we predict if the given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n",
    "\n",
    "The `submission.csv` file uses the following format:\n",
    "`id,target`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T20:05:13.000332Z",
     "iopub.status.busy": "2023-07-31T20:05:12.999248Z",
     "iopub.status.idle": "2023-07-31T20:05:13.023512Z",
     "shell.execute_reply": "2023-07-31T20:05:13.022432Z",
     "shell.execute_reply.started": "2023-07-31T20:05:13.000292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T20:05:13.025972Z",
     "iopub.status.busy": "2023-07-31T20:05:13.025189Z",
     "iopub.status.idle": "2023-07-31T20:05:28.765372Z",
     "shell.execute_reply": "2023-07-31T20:05:28.764404Z",
     "shell.execute_reply.started": "2023-07-31T20:05:13.025933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_submission[\"target\"] = np.argmax(classifier.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T20:05:28.767064Z",
     "iopub.status.busy": "2023-07-31T20:05:28.766701Z",
     "iopub.status.idle": "2023-07-31T20:05:28.787427Z",
     "shell.execute_reply": "2023-07-31T20:05:28.786399Z",
     "shell.execute_reply.started": "2023-07-31T20:05:28.767029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T20:05:28.789378Z",
     "iopub.status.busy": "2023-07-31T20:05:28.788974Z",
     "iopub.status.idle": "2023-07-31T20:05:28.803925Z",
     "shell.execute_reply": "2023-07-31T20:05:28.803021Z",
     "shell.execute_reply.started": "2023-07-31T20:05:28.78933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 869809,
     "sourceId": 17777,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30528,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
